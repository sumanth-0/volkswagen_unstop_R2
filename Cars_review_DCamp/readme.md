The CTO at "Car-ing is sharing", a car sales and rental company, hired you to help prototype a chatbot app that addresses diverse inquiries using LLMs. She proposed you piloting the following tasks:

Use a pre-trained LLM to classify the sentiment of the five car reviews in the car_reviews.csv dataset, and evaluate the classification accuracy and F1 score of predictions.
Store the model outputs in predicted_labels, then extract the labels and map them onto a list of {0,1} integer binary labels called predictions.
Store the calculated metrics in accuracy_result and f1_result.
The company is recently attracting customers from Spain. Extract and pass the first two sentences of the first review in the dataset to an English-to-Spanish translation LLM. Calculate the BLEU score to assess translation quality, using the content in reference_translations.txt as references.
Store the translated text generated by the LLM in translated_review.
Store the BLEU score metric result in bleu_score.
The 2nd review in the dataset emphasizes brand aspects. Load an extractive QA LLM such as "deepset/minilm-uncased-squad2" to formulate the question "What did he like about the brand?" and obtain an answer.
Use question and context for the two variables containing the LLM inputs: question and context.
Store the actual text answer in answer.
Summarize the last review in the dataset, into approximately 50-55 tokens long. Store it in the variable summarized_text.


How to approach the project
1. Classify car reviews

2. Translate a car review

3. Ask a question about a car review

4. Summarize and analyze a car review


Classify car reviews
You'll need to load a sentiment analysis LLM to classify the sentiment of each review in the dataset into POSITIVE or NEGATIVE, and utilize the real labels to calculate the accuracy and F1 score of predictions.


Choosing which model to use
The 'distilbert-base-uncased-finetuned-sst-2-english' model is a good choice of pre-trained LLM for binary sentiment classification.

Is this hint helpful?

Label mapping for metrics computation
In a text classification context, the accuracy and F1 score metrics in the evaluate library take two arguments: references, containing the ground truth labels, and predictions, containing the classification outputs produced by your model.
Before passing these two collections of labels to the metric for computation, it is necessary to map the categorical POSITIVE, NEGATIVE labels in the outputs and dataset ground-truth into numerical {0,1} labels.
Here is an example illustrating how to do it for the ground-truth labels: references = [1 if label == "POSITIVE" else 0 for label in real_labels]

Is this hint helpful?

2
Translate a car review
You'll need to load an English-to-Spanish translation LLM (for instance "Helsinki-NLP/opus-mt-en-es") to translate part of the first car review in the dataset into Spanish, and use the reference translations provided in a separate file to calculate the BLEU score.


Telling the translator LLM when to stop
Translation LLMs sequentially generate the translated output token by token.
As an alternative to manually extracting a priori the first two sentences from the original English review, you can play with the max_length argument when calling the model for inference. Try considering a maximum output length ranging between 25 and 30 tokens.

Is this hint helpful?

Preprocessing loaded reference translations
To remove special characters like "\n" once you read the txt file containing reference translations for the BLEU score computation, try the following: 
with open("data/reference_translations.txt", 'r') as file:
lines = file.readlines()
references = [line.strip() for line in lines]

Is this hint helpful?

3
Ask a question about a car review
You'll need to load a pre-trained LLM for extractive QA, such as "deepset/minilm-uncased-squad2", that takes two inputs (question and context) and extracts an answer directly from the context.


Output post-processing when using auto classes
If you opt for using auto classes for loading the model and tokenizer, you'll need to post-process the raw outputs by:
Identifying the start and end token positions of the answer, given by maximum logits in outputs.start_logits and outputs.end_logits (assuming your raw model outputs have been stored in outputs).
Extracting and decoding an answer span delimited by the previously identified start-end positions.

Is this hint helpful?

4
Summarize and analyze a car review
You'll need to load a summarization LLM and pass it the last car review in the dataset to generate a summary of approximately 50-55 tokens. You'll analyze if there are any potential biases in the generated summary text by calculating the maximum toxicity and regard metrics.


Formatting metric inputs as a list
When you extract the text of a single output generated by an LLM and pass it as an argument to the compute() method of the evaluate library, some metrics often require encapsulating that single string into a 1-element list.
For instance, use predictions=[summarized_text], with summarized_text as the string variable containing the extracted summarized text from the LLM output.

Is this hint helpful?

