{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n",
    "<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Raleway:wght@100&display=swap\" rel=\"stylesheet\">\n",
    "\n",
    "<h1 style=\"text-transform: uppercase; text-align: center; font-weight: 100;\">\n",
    "Regression in Tensorflow using RNN</h1>\n",
    "<p style=\"text-align: center; font-weight: 100;\">Predicting Volkswagen car prices</p><br/>\n",
    "\n",
    "Project requirements:\n",
    "- Only requirement is: minimum 10k rows\n",
    "- Any type of neural network - classical but convolutional and recurrent, better grade\n",
    "- Presentation of project, beside practical questions about model there would be also theoretical.\n",
    "The topic is free.\n",
    "\n",
    "Theory required for project presentation:\n",
    "- Practical side.\n",
    "- Discuss the scheme of an artificial neuron.\n",
    "- Activation function, why it is so important.\n",
    "- Explain how does neuron learn - steps, algorithm?\n",
    "- How does basic neural network work? (The more you know the better, as well as more complex).\n",
    "- Discuss the algorithm of neural network using backpropagation learning method. (metoda wstecznej propagacji błędów)\n",
    "- What subsets and why are the data divided into?\n",
    "Knowledge of ML, statistics."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data source: data were taken from the popular Polish automotive website Otomoto (https://www.otomoto.pl/).\n",
    "https://www.asimovinstitute.org/neural-network-zoo/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Import libraries and create dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn as sl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels as sm\n",
    "from scipy.stats import pearsonr\n",
    "import tensorflow as tf\n",
    "import keras.utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from tabulate import tabulate\n",
    "\n",
    "# from model import VolkswagenModel\n",
    "# from colorama import init, Fore, Style\n",
    "\n",
    "print(f\"numpy: {np.__version__}, pandas: {pd.__version__}, tensorflow: {tf.__version__}, matplotlib: {mpl.__version__}, seaborn: {sns.__version__}, statsmodels: {sm.__version__}, sklearn: {sl.__version__}\")\n",
    "print(\"Libs loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/otomoto.csv\")\n",
    "df = pd.DataFrame(data)\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "df.columns = ['Price', 'Year', 'Mileage', 'Tank capacity', 'Fuel type', 'Model', 'Estimation']\n",
    "print(\"Data technical info\")\n",
    "df.info()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "description = df.describe()\n",
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "check = 14915\n",
    "flag = True\n",
    "for i in range(description.shape[1]):\n",
    "    if description.iloc[0, i] != check:\n",
    "        print(\"Number of occurrences of data is not equal for every label.\")\n",
    "        print(f\"Problem at cell: (0, {i})\")\n",
    "        flag = False\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Checked: (0, {i})\")\n",
    "\n",
    "print(\"All columns passed\" if flag == True else \"Not passed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Clean data:\n",
    "\n",
    "    Data was cleaned previously in 'scratchpad.py' file and now all the records are represented by integers(documentation of each column values is located in 'model.py' file). Values ​​of 0 represent an error in reading data. Column \"Estimation\" contains a lot of 0 values, but this is due to the fact that not every article on the website contained such information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if df.isna().any().any():\n",
    "    df = df.dropna()\n",
    "    print(\"All rows with NaN values were dropped\")\n",
    "else:\n",
    "    print(\"0 NaN values\")\n",
    "\n",
    "cols_to_check = df.columns[df.columns != 'Estimation']\n",
    "if df[cols_to_check].eq(0).any().any():\n",
    "    df[cols_to_check] = df[cols_to_check].replace(0, np.nan)\n",
    "    print(\"All rows with 0 values were dropped\")\n",
    "else:\n",
    "    print(\"0 zero values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "Getting know data, analyzing dependencies\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_hist(col_name, bins_val):\n",
    "    min = df[col_name].min()\n",
    "    max = df[col_name].max()\n",
    "    print(f\"Lowest {col_name} value: {min}. Highest {col_name} value: {max}.\")\n",
    "    plot_hist = df[col_name].plot.hist(bins=bins_val, grid=True)\n",
    "    plot_hist.set_title(f\"Represents number of cars for each production {col_name.upper()} category\")\n",
    "    plot_hist.set_xlabel(f\"{col_name}\")\n",
    "    plot_hist.set_ylabel(\"Number of observations\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "make_hist(\"Price\", 57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "min = df[\"Year\"].min()\n",
    "max = df[\"Year\"].max()\n",
    "make_hist(\"Year\", max-min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "make_hist(\"Mileage\", 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "make_hist(\"Tank capacity\", 10) # default bins = 10\n",
    "\n",
    "# Trzeba jeszcze dopasować przedziały"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "make_hist(\"Fuel type\", 10) # default bins = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NO_cars_fuel = df['Fuel type'].value_counts().reset_index()\n",
    "NO_cars_fuel = NO_cars_fuel.rename(columns={'index': 'type of fuel', 'Fuel type': 'NO of cars with specific fuel type'})\n",
    "# NaN\n",
    "# d = {\"Benzyna\": 1, \"Benzyna+LPG\": 2, \"Benzyna+CNG\": 3, \"Elektryczny\": 4, \"Hybryda\": 5, \"Wodór\": 6, \"Diesel\": 7}\n",
    "# unique_values['name of fuel'] = unique_values['type of fuel'].map(d)\n",
    "NO_cars_fuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "make_hist(\"Model\", 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "make_hist(\"Estimation\", 10) # default bins = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Distribution of cars per column per certain category from column in numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pr = df['Price'].value_counts().reset_index()\n",
    "# yr = df['Year'].value_counts().reset_index()\n",
    "# mil = df['Mileage'].value_counts().reset_index()\n",
    "# tank = df['Tank capacity'].value_counts().reset_index()\n",
    "# fuel = df['Fuel type'].value_counts().reset_index()\n",
    "mod = df['Model'].value_counts().reset_index()\n",
    "est = df['Estimation'].value_counts().reset_index()\n",
    "\n",
    "# pr = pr.rename(columns={'index': 'price_cat', 'Price': 'NO_cars'})\n",
    "# yr = yr.rename(columns={'index': 'year_cat', 'Year': 'NO_cars'})\n",
    "# mil = mil.rename(columns={'index': 'mileage_cat', 'Mileage': 'NO_cars'})\n",
    "# tank = tank.rename(columns={'index': 'tank_capacity', 'Tank capacity': 'NO_cars'})\n",
    "# fuel = fuel.rename(columns={'index': 'type_fuel', 'Fuel type': 'NO_cars'})\n",
    "mod = mod.rename(columns={'index': 'type_model', 'Model': 'NO_cars'})\n",
    "est = est.rename(columns={'index': 'type_est', 'Estimation': 'NO_cars'})\n",
    "\n",
    "def create_table(table_data, table_title):\n",
    "    fig = plt.figure()\n",
    "    table = plt.table(cellText=table_data.values, colLabels=table_data.columns, loc='upper left')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1, 1.5)\n",
    "    plt.subplots_adjust(left=0.2, bottom=0.2)\n",
    "    plt.title(table_title)\n",
    "    plt.axis('off')\n",
    "    return fig\n",
    "\n",
    "# table1 = create_table(pr, \"Pr\")\n",
    "# table2 = create_table(yr, \"Yr\")\n",
    "# table3 = create_table(mil, \"Mil\")\n",
    "# table4 = create_table(tank, \"Tank\")\n",
    "# table5 = create_table(fuel, \"Fuel\")\n",
    "table6 = create_table(mod, \"Mod\")\n",
    "table7 = create_table(est, \"Est\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i, column in enumerate(df.columns):\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.boxplot(data=df[column], ax=ax).set(title=f\"{column.upper()} boxplot\", xlabel=f\"{column}\", ylabel=f\"Value of {column}\")\n",
    "    ax.set_title(column)\n",
    "    sns.despine()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Heat Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "correlation_matrix = np.corrcoef(df.values.T)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "sns.set(font_scale=1.1)\n",
    "sns.heatmap(data=correlation_matrix, square=True, cbar=True, annot=True, annot_kws={'size': 10}, xticklabels=df.columns, yticklabels=df.columns, fmt=\".2f\", linewidth=.5, cmap=sns.cubehelix_palette(as_cmap=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/durgeshrao9993/removing-outliers-by-iqr-method\n",
    "\n",
    "# # Interquartile Range (IQR)\n",
    "# Q1 = data.quantile(0.25)\n",
    "# Q3 = data.quantile(0.75)\n",
    "# IQR = Q3 - Q1\n",
    "# data = data[~((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# Calculate the z-scores of each column - measure that represents the number of standard deviations a data point is from the mean of the dataset\n",
    "z_scores = (df - df.mean()) / df.std()\n",
    "# Set the threshold for the z-score\n",
    "threshold = 3\n",
    "# Remove any rows where the z-score is greater than the threshold\n",
    "df = df[(np.abs(z_scores) < threshold).all(axis=1)]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"14915 dropped to {df.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Boxplot / Heat Map - after outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(df['Price']).set(title=\"PRICE boxplot\", xlabel=\"Price\", ylabel=\"Value of Price\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(df['Year']).set(title=\"YEAR boxplot\", xlabel=\"Year\", ylabel=\"Value of Year\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(df['Mileage']).set(title=\"MILEAGE boxplot\", xlabel=\"Mileage\", ylabel=\"Value of Mileage\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(df['Tank capacity']).set(title=\"TANK CAPACITY boxplot\", xlabel=\"Tank capacity\", ylabel=\"Value of Tank capacity\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "correlation_matrix = np.corrcoef(df.values.T)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "sns.set(font_scale=1.1)\n",
    "sns.heatmap(data=correlation_matrix, square=True, cbar=True, annot=True, annot_kws={'size': 10}, xticklabels=df.columns, yticklabels=df.columns, fmt=\".2f\", linewidth=.5, cmap=sns.cubehelix_palette(as_cmap=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pair plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.pairplot(df, height=1.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Splitting data into sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# SKLEARN\n",
    "\n",
    "X = df.drop(['Price'], axis=1)\n",
    "y = df['Price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# KERAS/TENSORFLOW\n",
    "\n",
    "# data_size = len(df)\n",
    "# test_size = int(data_size * 0.2)  # 20% danych przeznaczamy na zbiór testowy\n",
    "# train_size = data_size - test_size\n",
    "#\n",
    "# data = tf.data.Dataset.from_tensor_slices((X.values, y.values))\n",
    "# data = data.shuffle(buffer_size=data_size, reshuffle_each_iteration=True) # randomize data after each iteration (epoch)\n",
    "#\n",
    "# # Helps to optimize code by dividing datasets into batches, portion of data instead of individual processing.\n",
    "# train_data = data.take(train_size).batch(32)\n",
    "# test_data = data.skip(train_size).take(test_size).batch(32)\n",
    "# X_train, y_train = next(iter(train_data))\n",
    "# X_test, y_test = next(iter(test_data))\n",
    "#\n",
    "# # Print length of datasets portions\n",
    "# print(f\"X_train len = {len(X_train)}\")\n",
    "# print(f\"y_train len = {len(y_train)}\")\n",
    "# print(f\"X_test len = {len(X_test)}\")\n",
    "# print(f\"y_test len = {len(y_test)}\")\n",
    "#\n",
    "# # Print datasets portions\n",
    "# # print('X_train = ', X_train)\n",
    "# # print('y_train = ', y_train)\n",
    "# # print('X_test = ', X_test)\n",
    "# # print('y_test = ', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/guide/keras/rnn?hl=pl\n",
    "# https://www.ibm.com/topics/recurrent-neural-networks\n",
    "# Pipelines\n",
    "\n",
    "\n",
    "def LSTM_architecture(neurons_lstm, neurons_dense, activation, l2_val):\n",
    "    model_LSTM = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(neurons_lstm, input_shape=(1, X_train.shape[-1])),\n",
    "        tf.keras.layers.Dense(neurons_dense, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(l2_val)),\n",
    "        tf.keras.layers.Dense(neurons_dense, activation=activation),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model_LSTM.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mean_squared_error'\n",
    "    )\n",
    "    return model_LSTM\n",
    "\n",
    "def MLP_architecture(neurons_lstm, neurons_dense1, neurons_dense2, activation, l2_val):\n",
    "    modelMLP = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(neurons_lstm, input_shape=(X_train.shape[1],), activation=activation),\n",
    "        tf.keras.layers.Dropout(l2_val),\n",
    "        tf.keras.layers.Dense(neurons_dense1, activation=activation),\n",
    "        tf.keras.layers.Dropout(l2_val),\n",
    "        tf.keras.layers.Dense(neurons_dense2, activation=activation),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    return modelMLP\n",
    "\n",
    "def GRU_architecture(neurons_lstm=128, neurons_dense=64, activation='relu', l2_val=0.01):\n",
    "    model_GRU = tf.keras.Sequential([\n",
    "        tf.keras.layers.GRU(neurons_lstm, input_shape=(1, X_train.shape[-1])),\n",
    "        tf.keras.layers.Dense(neurons_dense, activation=activation , kernel_regularizer=tf.keras.regularizers.l2(l2_val)),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model_GRU.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse'\n",
    "    )\n",
    "    return model_GRU\n",
    "\n",
    "def MLP_architecture_v2(neurons_lstm, neurons_dense1, neurons_dense2, neurons_dense3, neurons_dense4, activation, l2_val):\n",
    "    modelMLPv2 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(neurons_lstm, input_shape=(X_train.shape[1],), activation=activation),\n",
    "        tf.keras.layers.Dropout(l2_val),\n",
    "        tf.keras.layers.Dense(neurons_dense1, activation=activation),\n",
    "        tf.keras.layers.Dropout(l2_val),\n",
    "        tf.keras.layers.Dense(neurons_dense2, activation=activation),\n",
    "        tf.keras.layers.Dropout(l2_val),\n",
    "        tf.keras.layers.Dense(neurons_dense3, activation=activation),\n",
    "        tf.keras.layers.Dropout(l2_val),\n",
    "        tf.keras.layers.Dense(neurons_dense4, activation=activation),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    modelMLPv2.compile(loss='mse', optimizer='adam')\n",
    "    return modelMLPv2\n",
    "\n",
    "\n",
    "# Equivalent to:\n",
    "# input_layer = tf.keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "# lstm_layer = tf.keras.layers.LSTM(64)(input_layer)\n",
    "# dense_layer_1 = tf.keras.layers.Dense(32, activation='relu')(lstm_layer)\n",
    "# output_layer = tf.keras.layers.Dense(1)(dense_layer_1)\n",
    "# model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Normalize / Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Normalization - we change the distribution of data, not the range of data like it is done in scaling. The point is to change observations so that they can be described as a normal distribution.\n",
    "# Normalization should be done after splitting into train and test data sets.\n",
    "# https://keras.io/api/layers/preprocessing_layers/numerical/normalization/\n",
    "# fit_transform() is used to learn and apply the transformation to the data in one step, whereas transform() is used to apply the transformation to new data using the learned parameters.\n",
    "\n",
    "# If you are using a regression model and want to predict a continuous target variable (i.e., y), then you should scale both the input features (X) and the target variable (y). This is because scaling both the input features and target variable will help the model to learn better and converge faster.\n",
    "# If you are using a classification model and want to predict a binary or categorical target variable, then you do not need to scale the target variable (y). This is because scaling does not change the nature of the target variable, and most classification algorithms do not depend on the scale of the target variable.\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))\n",
    "print(\"Normalization done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compile and train training the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reshape 2D array into 3D tensor\n",
    "X_train_3D = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_3D = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_LSTM = LSTM_architecture(neurons_lstm=64, neurons_dense=32, activation='relu', l2_val=0.01)\n",
    "\n",
    "\n",
    "# \"History\" object contains information about the training process, including the loss and metrics values recorded during training and validation at each epoch.\n",
    "history_LSTM = model_LSTM.fit(\n",
    "    X_train_3D,\n",
    "    y_train_scaled,\n",
    "    epochs=5,\n",
    "    validation_data=(X_test_3D, y_test_scaled),\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_MLP = MLP_architecture(neurons_lstm=128, neurons_dense1=64, neurons_dense2=32, activation='relu', l2_val=0.01)\n",
    "model_MLP.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse'\n",
    ")\n",
    "# \"History\" object contains information about the training process, including the loss and metrics values recorded during training and validation at each epoch.\n",
    "history_MLP = model_MLP.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_scaled,\n",
    "    epochs=5,\n",
    "    validation_data=(X_test_scaled, y_test_scaled),\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_GRU = GRU_architecture(neurons_lstm=128, neurons_dense=64, activation='relu', l2_val=0.01)\n",
    "\n",
    "\n",
    "# \"History\" object contains information about the training process, including the loss and metrics values recorded during training and validation at each epoch.\n",
    "history_GRU = model_GRU.fit(\n",
    "    X_train_3D,\n",
    "    y_train_scaled,\n",
    "    epochs=5,\n",
    "    validation_data=(X_test_3D, y_test_scaled),\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_MLP_more_neurons = MLP_architecture(neurons_lstm=128, neurons_dense1=128, neurons_dense2=64, activation='relu', l2_val=0.01)\n",
    "model_MLP_more_neurons.compile(\n",
    "optimizer = 'adam',\n",
    "loss = 'mse'\n",
    ")\n",
    "# \"History\" object contains information about the training process, including the loss and metrics values recorded during training and validation at each epoch.\n",
    "history_MLP_more_neurons = model_MLP_more_neurons.fit(\n",
    "X_train_scaled,\n",
    "y_train_scaled,\n",
    "epochs = 5,\n",
    "validation_data = (X_test_scaled, y_test_scaled),\n",
    "batch_size = 32,\n",
    "validation_split = 0.2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_MLP_v2 = MLP_architecture_v2(neurons_lstm=128, neurons_dense1=64, neurons_dense2=32, neurons_dense3=16, neurons_dense4=16, activation='relu', l2_val=0.01)\n",
    "model_MLP_v2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse'\n",
    ")\n",
    "history_MLP_v2 = model_MLP_v2.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_scaled,\n",
    "    epochs=5,\n",
    "    validation_data=(X_test_scaled, y_test_scaled),\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We could also add combination of MLP and CNN."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluating the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"---------- LSTM MODEL ----------\")\n",
    "\n",
    "test_loss_LSTM = model_LSTM .evaluate(X_test_3D, y_test_scaled)\n",
    "train_loss_LSTM = model_LSTM.evaluate(X_train_3D, y_train_scaled)\n",
    "\n",
    "y_train_pred_LSTM_scaled = model_LSTM.predict(X_train_3D)\n",
    "y_test_pred_LSTM_scaled = model_LSTM.predict(X_test_3D)\n",
    "\n",
    "y_train_pred_LSTM = scaler.inverse_transform(y_train_pred_LSTM_scaled)\n",
    "y_test_pred_LSTM = scaler.inverse_transform(y_test_pred_LSTM_scaled)\n",
    "\n",
    "rmse_train_LSTM = np.sqrt(mean_squared_error(y_train, y_train_pred_LSTM))\n",
    "rmse_test_LSTM = np.sqrt(mean_squared_error(y_test, y_test_pred_LSTM))\n",
    "\n",
    "mae_train_LSTM = mean_absolute_error(y_train, y_train_pred_LSTM)\n",
    "mae_test_LSTM = mean_absolute_error(y_test, y_test_pred_LSTM)\n",
    "\n",
    "corr_train_LSTM = np.corrcoef(y_train.T, y_train_pred_LSTM.T)[0, 1]\n",
    "corr_test_LSTM = np.corrcoef(y_test.T, y_test_pred_LSTM.T)[0, 1]\n",
    "\n",
    "mpe_train_LSTM = mean_absolute_percentage_error(y_train, y_train_pred_LSTM)\n",
    "mpe_test_LSTM = mean_absolute_percentage_error(y_test, y_test_pred_LSTM)\n",
    "\n",
    "print(\"LSTM done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"---------- MLP MODEL ----------\")\n",
    "\n",
    "test_loss_MLP = model_MLP.evaluate(X_test_scaled, y_test_scaled)\n",
    "train_loss_MLP = model_MLP.evaluate(X_train_scaled, y_train_scaled)\n",
    "\n",
    "y_train_pred_MLP_scaled = model_MLP.predict(X_train_scaled)\n",
    "y_test_pred_MLP_scaled = model_MLP.predict(X_test_scaled)\n",
    "\n",
    "y_train_pred_MLP = scaler.inverse_transform(y_train_pred_MLP_scaled)\n",
    "y_test_pred_MLP = scaler.inverse_transform(y_test_pred_MLP_scaled)\n",
    "\n",
    "rmse_train_MLP = np.sqrt(mean_squared_error(y_train, y_train_pred_MLP))\n",
    "rmse_test_MLP = np.sqrt(mean_squared_error(y_test, y_test_pred_MLP))\n",
    "\n",
    "mae_train_MLP = mean_absolute_error(y_train, y_train_pred_MLP)\n",
    "mae_test_MLP = mean_absolute_error(y_test, y_test_pred_MLP)\n",
    "\n",
    "corr_train_MLP = np.corrcoef(y_train.T, y_train_pred_MLP.T)[0, 1]\n",
    "corr_test_MLP = np.corrcoef(y_test.T, y_test_pred_MLP.T)[0, 1]\n",
    "\n",
    "mpe_train_MLP = mean_absolute_percentage_error(y_train, y_train_pred_MLP)\n",
    "mpe_test_MLP = mean_absolute_percentage_error(y_test, y_test_pred_MLP)\n",
    "\n",
    "print(\"MLP done\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"---------- GRU MODEL ----------\")\n",
    "test_loss_GRU = model_GRU .evaluate(X_test_3D, y_test_scaled)\n",
    "train_loss_GRU = model_GRU.evaluate(X_train_3D, y_train_scaled)\n",
    "\n",
    "y_train_pred_GRU_scaled = model_GRU.predict(X_train_3D)\n",
    "y_test_pred_GRU_scaled = model_GRU.predict(X_test_3D)\n",
    "\n",
    "y_train_pred_GRU = scaler.inverse_transform(y_train_pred_GRU_scaled)\n",
    "y_test_pred_GRU = scaler.inverse_transform(y_test_pred_GRU_scaled)\n",
    "\n",
    "rmse_train_GRU = np.sqrt(mean_squared_error(y_train, y_train_pred_GRU))\n",
    "rmse_test_GRU = np.sqrt(mean_squared_error(y_test, y_test_pred_GRU))\n",
    "\n",
    "mae_train_GRU = mean_absolute_error(y_train, y_train_pred_GRU)\n",
    "mae_test_GRU = mean_absolute_error(y_test, y_test_pred_GRU)\n",
    "\n",
    "corr_train_GRU= np.corrcoef(y_train.T, y_train_pred_GRU.T)[0, 1]\n",
    "corr_test_GRU = np.corrcoef(y_test.T, y_test_pred_GRU.T)[0, 1]\n",
    "\n",
    "mpe_train_GRU = mean_absolute_percentage_error(y_train, y_train_pred_GRU)\n",
    "mpe_test_GRU = mean_absolute_percentage_error(y_test, y_test_pred_GRU)\n",
    "print(\"GRU done\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"---------- MLP_more_neurons MODEL ----------\")\n",
    "\n",
    "test_loss_MLP_n = model_MLP_more_neurons.evaluate(X_test_scaled, y_test_scaled)\n",
    "train_loss_MLP_n = model_MLP_more_neurons.evaluate(X_train_scaled, y_train_scaled)\n",
    "\n",
    "y_train_pred_MLP_n_scaled = model_MLP_more_neurons.predict(X_train_scaled)\n",
    "y_test_pred_MLP_n_scaled = model_MLP_more_neurons.predict(X_test_scaled)\n",
    "\n",
    "y_train_pred_MLP_n = scaler.inverse_transform(y_train_pred_MLP_n_scaled)\n",
    "y_test_pred_MLP_n = scaler.inverse_transform(y_test_pred_MLP_n_scaled)\n",
    "\n",
    "rmse_train_MLP_n = np.sqrt(mean_squared_error(y_train, y_train_pred_MLP_n))\n",
    "rmse_test_MLP_n = np.sqrt(mean_squared_error(y_test, y_test_pred_MLP_n))\n",
    "\n",
    "mae_train_MLP_n = mean_absolute_error(y_train, y_train_pred_MLP_n)\n",
    "mae_test_MLP_n = mean_absolute_error(y_test, y_test_pred_MLP_n)\n",
    "\n",
    "corr_train_MLP_n = np.corrcoef(y_train.T, y_train_pred_MLP_n.T)[0, 1]\n",
    "corr_test_MLP_n = np.corrcoef(y_test.T, y_test_pred_MLP_n.T)[0, 1]\n",
    "\n",
    "mpe_train_MLP_n = mean_absolute_percentage_error(y_train, y_train_pred_MLP_n)\n",
    "mpe_test_MLP_n = mean_absolute_percentage_error(y_test, y_test_pred_MLP_n)\n",
    "print(\"MLP_more_neurons done\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"---------- MLP_v2 MODEL ----------\")\n",
    "\n",
    "test_loss_MLP_v2 = model_MLP_v2.evaluate(X_test_scaled, y_test_scaled)\n",
    "train_loss_MLP_v2 = model_MLP_v2.evaluate(X_train_scaled, y_train_scaled)\n",
    "\n",
    "y_train_pred_MLP_v2_scaled = model_MLP_v2.predict(X_train_scaled)\n",
    "y_test_pred_MLP_v2_scaled = model_MLP_v2.predict(X_test_scaled)\n",
    "\n",
    "y_train_pred_MLP_v2 = scaler.inverse_transform(y_train_pred_MLP_v2_scaled)\n",
    "y_test_pred_MLP_v2 = scaler.inverse_transform(y_test_pred_MLP_v2_scaled)\n",
    "\n",
    "rmse_train_MLP_v2 = np.sqrt(mean_squared_error(y_train, y_train_pred_MLP_v2))\n",
    "rmse_test_MLP_v2 = np.sqrt(mean_squared_error(y_test, y_test_pred_MLP_v2))\n",
    "\n",
    "mae_train_MLP_v2 = mean_absolute_error(y_train, y_train_pred_MLP_v2)\n",
    "mae_test_MLP_v2 = mean_absolute_error(y_test, y_test_pred_MLP_v2)\n",
    "\n",
    "corr_train_MLP_v2 = np.corrcoef(y_train.T, y_train_pred_MLP_v2.T)[0, 1]\n",
    "corr_test_MLP_v2 = np.corrcoef(y_test.T, y_test_pred_MLP_v2.T)[0, 1]\n",
    "\n",
    "mpe_train_MLP_v2 = mean_absolute_percentage_error(y_train, y_train_pred_MLP_v2)\n",
    "mpe_test_MLP_v2 = mean_absolute_percentage_error(y_test, y_test_pred_MLP_v2)\n",
    "print('MLP_v2 done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LSTM_eval = [test_loss_LSTM, train_loss_LSTM, rmse_test_LSTM, rmse_train_LSTM, mae_test_LSTM,mae_train_LSTM, corr_test_LSTM, corr_train_LSTM, mpe_test_LSTM, mpe_train_LSTM]\n",
    "MLP_eval = [test_loss_MLP, train_loss_MLP, rmse_test_MLP, rmse_train_MLP, mae_test_MLP, mae_train_MLP, corr_test_MLP, corr_train_MLP, mpe_test_MLP, mpe_train_MLP]\n",
    "GRU_eval = [test_loss_GRU, train_loss_GRU, rmse_test_GRU, rmse_train_GRU, mae_test_GRU, mae_train_GRU, corr_test_GRU, corr_train_GRU, mpe_test_GRU, mpe_train_GRU]\n",
    "MLP_n_eval = [test_loss_MLP_n, train_loss_MLP_n, rmse_test_MLP_n, rmse_train_MLP_n, mae_test_MLP_n, mae_train_MLP_n, corr_test_MLP_n, corr_train_MLP_n, mpe_test_MLP_n, mpe_train_MLP_n]\n",
    "MLP_v2_eval = [test_loss_MLP_v2, train_loss_MLP_v2, rmse_test_MLP_v2, rmse_train_MLP_v2, mae_test_MLP_v2, mae_train_MLP_v2, corr_test_MLP_v2, corr_train_MLP_v2, mpe_test_MLP_v2, mpe_train_MLP_v2]\n",
    "\n",
    "evaluation_headers = ['Test Loss', 'Train Loss', 'Test RMSE', 'Train RMSE', 'Test MAE', 'Train MAE', 'Test Pearson Coef', 'Train Pearson Coef', \"Test % Error\", 'Train % Error']\n",
    "evaldf = pd.DataFrame({'Model': evaluation_headers,\n",
    "                     'LSTM': LSTM_eval,\n",
    "                     'MLP': MLP_eval,\n",
    "                     'GRU': GRU_eval,\n",
    "                     'MLP_n': MLP_n_eval,\n",
    "                     'MLP_v2': MLP_v2_eval\n",
    "                     })\n",
    "evaldf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Metrics plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loss_plots(model_history, model_name):\n",
    "    # Train and validation loss history\n",
    "    train_loss = model_history.history['loss']\n",
    "    val_loss = model_history.history['val_loss']\n",
    "\n",
    "    # Plot the training and validation loss\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.title(f'Training and Validation Loss FOR {model_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def pred_v_actual_plots(y_pred_model, model_name):\n",
    "    # Predictions vs. actual values\n",
    "    plt.scatter(y_test, y_pred_model, color='red', label='Predicted')\n",
    "    plt.scatter(y_test, y_test, color='blue', label='Actual')\n",
    "    plt.title(f'Predictions vs. Actual Values FOR {model_name}')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def mae_rmse_plots(y_pred_model, model_name, mae_model, rmse_model):\n",
    "    y_pred_model_flatten = y_pred_model.flatten()\n",
    "    # Plot the Mean Absolute Error\n",
    "    plt.scatter(y_test, abs(y_pred_model_flatten - y_test), color='cyan', label='Mean Absolute Error')\n",
    "    plt.axhline(y=mae_model, color='brown', linestyle='--', label='MAE')\n",
    "    plt.title(f'Mean Absolute Error FOR {model_name}')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the Root Mean Square Deviation\n",
    "    plt.scatter(y_test, (y_pred_model_flatten - y_test)**2, color='purple', label='Root Mean Square Error')\n",
    "    plt.axhline(y=rmse_model, color='yellow', linestyle='--', label='RMSE')\n",
    "    plt.title(f'Mean Squared Error FOR {model_name}')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# https://www.baeldung.com/cs/training-validation-loss-deep-learning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_plots(history_LSTM, \"LSTM\")\n",
    "loss_plots(history_MLP, \"MLP\")\n",
    "loss_plots(history_GRU, \"GRU\")\n",
    "loss_plots(history_MLP_more_neurons, \"MLP_denser\")\n",
    "# loss_plots(history_MLP_v2, \"MLP_v2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred_v_actual_plots(y_test_pred_LSTM, \"LSTM\")\n",
    "pred_v_actual_plots(y_test_pred_MLP, \"MLP\")\n",
    "pred_v_actual_plots(y_test_pred_GRU, \"GRU\")\n",
    "pred_v_actual_plots(y_test_pred_MLP_n, \"MLP_denser\")\n",
    "pred_v_actual_plots(y_test_pred_MLP_v2, \"MLP_v2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mae_rmse_plots(y_test_pred_LSTM, \"LSTM\", mae_test_LSTM, rmse_test_LSTM)\n",
    "mae_rmse_plots(y_test_pred_MLP, \"MLP\", mae_test_MLP, rmse_test_MLP)\n",
    "mae_rmse_plots(y_test_pred_GRU, \"GRU\", mae_test_GRU, rmse_test_GRU)\n",
    "mae_rmse_plots(y_test_pred_MLP_n, \"MLP_denser\", mae_test_MLP_n, rmse_test_MLP_n)\n",
    "mae_rmse_plots(y_test_pred_MLP_v2, \"MLP_v2\", mae_test_MLP_v2, rmse_test_MLP_v2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Get the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: to fix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "#\n",
    "# from keras.wrappers.scikit_learn import KerasRegressor\n",
    "#\n",
    "# # keras tuner\n",
    "# # błąd procentowy\n",
    "#\n",
    "#\n",
    "# def get_hyperparameters():\n",
    "#         model = KerasRegressor(build_fn=LSTM_architecture, verbose=0)\n",
    "#         param_grid = {\n",
    "#             'neurons_lstm': [32, 64, 128],\n",
    "#             'neurons_dense': [16, 32, 64],\n",
    "#             'activation': ['relu'],\n",
    "#             'l2_val': [0.01, 0.001, 0.0001]\n",
    "#         }\n",
    "#         searcher = RandomizedSearchCV(\n",
    "#             estimator=model,\n",
    "#             param_distributions=param_grid,\n",
    "#             n_iter=5,\n",
    "#             cv=3,\n",
    "#             n_jobs=-1\n",
    "#         )\n",
    "#         searcher.fit(X_train_3D, y_train_scaled)\n",
    "#         return searcher.best_params_, searcher.best_score_\n",
    "#\n",
    "# # Call the function to get the best hyperparameters\n",
    "# best_params, best_score = get_hyperparameters()\n",
    "#\n",
    "# print(\"Best parameters: \", best_params)\n",
    "# print(\"Best score: \", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Every possible combination of param_grid parameters.\n",
    "TODO: Best parameters:  {'neurons_lstm': 64, 'neurons_dense': 32, 'l2_val': 0.0001, 'activation': 'relu'}\n",
    "Best score:  -0.21322791775067648 <- completely wrong"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "#\n",
    "# # definiowanie zestawów parametrów do przetestowania\n",
    "# param_grid = {\n",
    "#     'neurons_lstm': [64, 128],\n",
    "#     'neurons_dense1': [32, 64],\n",
    "#     'neurons_dense2': [16, 32],\n",
    "#     'neurons_dense3': [8, 16],\n",
    "#     'neurons_dense4': [8, 16],\n",
    "#     'activation': ['relu', 'tanh'],\n",
    "#     'l2_val': [0.01, 0.001],\n",
    "#     'batch_size': [32, 64]\n",
    "# }\n",
    "#\n",
    "#\n",
    "# model = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=MLP_architecture_v2, epochs=1, verbose=0)\n",
    "#\n",
    "# # start Grid Search\n",
    "# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "# grid_result = grid_search.fit(X_train_scaled, y_train_scaled)\n",
    "#\n",
    "# print(f\"Best score: {grid_result.best_score_}\")\n",
    "# print(f\"Best params: {grid_result.best_params_}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Possible DEPLOY\n",
    "To deploy code in form of Docker we should probably enclose model into some kind of REST API for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}