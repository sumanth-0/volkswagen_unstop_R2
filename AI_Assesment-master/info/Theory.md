# PROJECT THEORY

## Scheme of an artificial neuron



<p style="text-transform: uppercase; text-align: 
center"><mark style="background-color: #360410;">input signal</mark></p>
<p style="text-align: center">Numeric values as an argument for neural network.</p>
<p style="text-transform: uppercase; text-align: 
center">v</p>
<p style="text-transform: uppercase; text-align: center"><mark  style="background-color: #360410;">weights (w)</mark></p>
<p style="text-transform: uppercase; text-align: 
center">v</p>
<p style="text-transform: uppercase; text-align: center"><mark style="background-color: #360410;">weighted sum</mark></p>
<p style="text-transform: uppercase; text-align: 
center">v</p>
<p style="text-transform: uppercase; text-align: center"><mark style="background-color: #360410;">activation function</mark></p>
<p style="text-transform: uppercase; text-align: 
center">v</p>
<p style="text-transform: uppercase; text-align: center"><mark style="background-color: #360410;">output signal</mark></p>

## Activation function

This feature introduces non-linearity to the model, which allows you to model more complex relationships between inputs and outputs. Without the activation function, a neural network would be a linear regression model, and incapable of modeling more complex relationships. It is used in every layer except the input one.

In the case of regression, a linear activation function or no activation function is often used in the output layer of the neural network.

#### How does the activation functions work? 

<mark>ReLu</mark>
```rust
f(x) = max(0, x) 
```
For each input, if the input value is greater than 0, the output of the function is equal to the input values. 

<mark>Sigmoid</mark>
```rust
f(x) = 1 / (1 + exp(-x))
```
This function takes input values ​​in the range (-infinity, +infinity) and returns output values ​​in the range (0, 1), which can be interpreted as probability.

<mark>Tanh</mark>
```rust
f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))
```
It takes input values ​​in the range (-infinity, +infinity) and returns output values ​​in the range (-1, 1).

#### Example of usage

```python

```


## How does neuron learn? 

#### Backpropagation

During backpropagation, errors generated by the output models are passed back through the network to the input layers. In each layer, errors are calculated and passed back to the previous layer for further error calculation. This process continues until the input layers are reached, and then the weights are updated to minimize errors. These weights are updated using a learning optimizer such as SGD (stochastic gradient descent) or variations thereof.

#### Forward propagation

During forward propagation, input data is passed through successive layers of the network until the output layer, which generates predictions or model results. In each layer, the input values ​​are transformed by the activation function and passed to the next layer as input. This process continues until the output layer that generates the predictions or model results is reached.

## How does neural network work?

<p style="text-transform: uppercase; text-align: 
center">Initialization of weights and biases</p>
<p style="text-transform: uppercase; text-align: 
center">v</p>
<p style="text-transform: uppercase; text-align: center">Signal transfer</p>
<p style="text-transform: uppercase; text-align: 
center">v</p>
<p style="text-transform: uppercase; text-align: center">Error calculation</p>
<p style="text-transform: uppercase; text-align: 
center">v</p>
<p style="text-transform: uppercase; text-align: center">Backpropagation</p>
<p style="text-transform: uppercase; text-align: 
center">v</p>
<p style="text-transform: uppercase; text-align: center">Actualization of weights and biases</p>
<p style="text-transform: uppercase; text-align: 
center">v</p>
<p style="text-transform: uppercase; text-align: center">Repeating the signal</p>

## Subsets in ML

#### Training subset



#### Validation subset

